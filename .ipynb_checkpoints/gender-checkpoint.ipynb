{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def plot_loss_acc(history):\n",
    "    '''\n",
    "    Plots the training and validation accuracy and loss curves for fitted keras models\n",
    "    In: The history object of a trained keras model\n",
    "    Out: 2 Plots (Accuracy & Loss)\n",
    "    '''\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))  \n",
    "    fig.suptitle('Model Performance')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Training and Validation Accuracy')\n",
    "    ax1.set(xlabel='Epoch', ylabel='Accuracy')    \n",
    "    ax1.legend(['Training Accuracy', 'Validation Accuracy'], loc='best')\n",
    "    \n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Training and Validation Loss')\n",
    "    ax2.set(xlabel='Epoch', ylabel='Loss') \n",
    "    ax2.legend(['Training Loss', 'Validation Loss'], loc='best')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_conf_mat(model, load=False, load_loc=None, class_lst = 'Angry Disgust Fear Happy Sad Surprise Neutral', out='model_weights/finalconf.png'):\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import math\n",
    "    # 0=angry, 1=disgust,2=fear,3=happy, 4=sad, 5=surprise, 6=neutral\n",
    "    classes = class_lst.split(' ')\n",
    "    \n",
    "    number_of_examples = len(validation_generator.filenames)\n",
    "    number_of_generator_calls = math.ceil(number_of_examples / (1.0 * batch_size)) \n",
    "    # 1.0 above is to skip integer division\n",
    "    y_true = []\n",
    "    for i in range(0,int(number_of_generator_calls)):\n",
    "        y_true.extend(np.array(validation_generator[i][1]))\n",
    "        \n",
    "    print(len(y_true))\n",
    "    print('ones', sum(y_true))\n",
    "    #y_true=[np.argmax(x) for x in val_data[1]]\n",
    "\n",
    "    if load:\n",
    "        model.load_weights(load_loc)\n",
    "        mod_name = load_loc.split('-')[0]\n",
    "    else:\n",
    "        mod_name=out\n",
    "\n",
    "        \n",
    "    val_data = []\n",
    "    for i in range(0,int(number_of_generator_calls)):\n",
    "        val_data.extend(np.array(validation_generator[i][0]))\n",
    "    \n",
    "    val_data = np.array(val_data)\n",
    "    \n",
    "    print('Prediction Time')\n",
    "    y_pred=np.argmax(model.predict(val_data),axis=1)\n",
    "    \n",
    "    \n",
    "    con_mat = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()\n",
    "\n",
    "    con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                         index = classes, \n",
    "                         columns = classes)\n",
    "\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    ax = sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.BuGn)\n",
    "    ax.set_ylim(len(classes),0)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.gcf().subplots_adjust(bottom=0.15, left=0.15)\n",
    "    plt.savefig(mod_name+'_performance.png')\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 4.0 #was 2.0\n",
    "    return x\n",
    "\n",
    "def split_data(x, y, validation_split=.2):\n",
    "    num_samples = len(x)\n",
    "    num_train_samples = int((1 - validation_split)*num_samples)\n",
    "    train_x = x[:num_train_samples]\n",
    "    train_y = y[:num_train_samples]\n",
    "    val_x = x[num_train_samples:]\n",
    "    val_y = y[num_train_samples:]\n",
    "    train_data = (train_x, train_y)\n",
    "    val_data = (val_x, val_y)\n",
    "    return train_data, val_data\n",
    "\n",
    "def split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):\n",
    "    ground_truth_keys = sorted(ground_truth_data.keys())\n",
    "    if do_shuffle is not False:\n",
    "        shuffle(ground_truth_keys)\n",
    "    training_split = 1 - validation_split\n",
    "    num_train = int(training_split * len(ground_truth_keys))\n",
    "    train_keys = ground_truth_keys[:num_train]\n",
    "    validation_keys = ground_truth_keys[num_train:]\n",
    "    return train_keys, validation_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from tensorflow.keras.callbacks import   # CALLBACKS\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# \n",
    "# from tensorflow.keras.layers import Flatten, Dense\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tqdm.keras import TqdmCallback\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import cv2\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Parameter Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "image_size=(64, 64)\n",
    "image_width, image_height = 64, 64\n",
    "num_classes = 2\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "dataset_name = 'imdb'\n",
    "validation_split = .1\n",
    "\n",
    "# if K.image_data_format() == 'channels_first':\n",
    "#     input_shape = (1, image_width, image_height)\n",
    "# else:\n",
    "#     input_shape = (image_width, image_height, 1)\n",
    "\n",
    "\n",
    "do_random_crop = False\n",
    "patience = 100\n",
    "dataset_name = 'imdb'\n",
    "input_shape = (64, 64, 3)\n",
    "if input_shape[2] == 1:\n",
    "    grayscale = True\n",
    "    \n",
    "images_path = 'data/imdb_crop/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_data_dir = 'data/gender/train/'\n",
    "validation_data_dir = 'data/gender/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def count_num_files(root=None):\n",
    "    import os\n",
    "    count=0\n",
    "    for path, subdirs, files in os.walk(root):\n",
    "        for name in files:\n",
    "            count+=1\n",
    "        \n",
    "    return count\n",
    "\n",
    "nb_train_samples = 171098 #count_num_files('data/dataset/gender/train/')\n",
    "nb_validation_samples = 53742 #count_num_files('data/dataset/gender/test/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# face_score_treshold = 3\n",
    "# dataset = loadmat('data/imdb_crop/imdb.mat')\n",
    "# image_names_array = dataset['imdb']['full_path'][0, 0][0]\n",
    "# gender_classes = dataset['imdb']['gender'][0, 0][0]\n",
    "# face_score = dataset['imdb']['face_score'][0, 0][0]\n",
    "# second_face_score = dataset['imdb']['second_face_score'][0, 0][0]\n",
    "# face_score_mask = face_score > face_score_treshold\n",
    "# second_face_score_mask = np.isnan(second_face_score)\n",
    "# unknown_gender_mask = np.logical_not(np.isnan(gender_classes))\n",
    "# mask = np.logical_and(face_score_mask, second_face_score_mask)\n",
    "# mask = np.logical_and(mask, unknown_gender_mask)\n",
    "# image_names_array = image_names_array[mask]\n",
    "# gender_classes = gender_classes[mask].tolist()\n",
    "# image_names = []\n",
    "# for image_name_arg in range(image_names_array.shape[0]):\n",
    "#     image_name = image_names_array[image_name_arg][0]\n",
    "#     image_names.append(image_name)\n",
    "    \n",
    "    \n",
    "# ground_truth_data = dict(zip(image_names, gender_classes))\n",
    "\n",
    "# train_keys, val_keys = split_imdb_data(ground_truth_data, validation_split)\n",
    "# print('Number of training samples:', len(train_keys))\n",
    "# print('Number of validation samples:', len(val_keys))\n",
    "# train_dict = { train_key: ground_truth_data[train_key] for train_key in train_keys }\n",
    "# val_dict = { val_key: ground_truth_data[val_key] for val_key in val_keys }\n",
    "# #dict_you_want = { your_key: old_dict[your_key] for your_key in your_keys }\n",
    "# train_data_files = list(train_dict.keys())\n",
    "# train_labels = np.array(list(train_dict.values()))\n",
    "\n",
    "\n",
    "# val_data_files = list(val_dict.keys())\n",
    "# val_labels = np.array(list(val_dict.values()))\n",
    "\n",
    "# #import pandas as pd\n",
    "# #train_df = pd.DataFrame(columns=['img', 'label'])\n",
    "# def get_images(filenames, labels, data_dir='data/imdb_crop/', image_size=(64,64)):\n",
    "#     #df = pd.DataFrame(columns=['img'])\n",
    "#     a = []\n",
    "#     ctr=0\n",
    "#     for i, datapoint in enumerate(filenames):\n",
    "#         image_array = imread(data_dir+datapoint)\n",
    "#         image_array = imresize(image_array, image_size)\n",
    "#         image_array = cv2.cvtColor(\n",
    "#         image_array.astype('uint8'),\n",
    "#         cv2.COLOR_RGB2GRAY).astype('float32')\n",
    "#         image_array = np.expand_dims(image_array, -1)\n",
    "#         #df2=pd.DataFrame({'img':image_array}, index=[0])\n",
    "#         #df.append(df2, ignore_index=True)\n",
    "#         a.append(image_array)\n",
    "#     imgs = np.array(a)\n",
    "#     return imgs\n",
    "# train_imgs= get_images(train_data_files, train_labels)\n",
    "# val_imgs = get_images(train_data_files, train_labels)\n",
    "\n",
    "# val_labels.shape\n",
    "\n",
    "# from data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171098 images belonging to 2 classes.\n",
      "Found 53742 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# image_generator = ImageGenerator(ground_truth_data, batch_size,\n",
    "#                                  input_shape[:2],\n",
    "#                                  train_keys, val_keys, None,\n",
    "#                                  path_prefix=images_path,\n",
    "#                                  vertical_flip_probability=0,\n",
    "#                                  grayscale=grayscale,\n",
    "#                                  do_random_crop=do_random_crop)\n",
    "\n",
    "# data_generator = ImageDataGenerator(\n",
    "#                         #rescale=1. / 255,\n",
    "#                         featurewise_center=False,\n",
    "#                         featurewise_std_normalization=False,\n",
    "#                         rotation_range=10,\n",
    "#                         width_shift_range=0.1,\n",
    "#                         height_shift_range=0.1,\n",
    "#                         zoom_range=.1,\n",
    "#                         horizontal_flip=True)\n",
    "# test_datagen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "                        rescale=1. / 255,\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(image_width, image_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## MODEL: Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# simple cnn\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=(7, 7), padding='same',\n",
    "                        name='image_array', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "model.add(Dropout(.5))\n",
    "\n",
    "model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(\n",
    "    filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Activation('softmax', name='predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# simple callbacks\n",
    "log_file_path = 'logs/' + dataset_name + '_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "\n",
    "trained_models_path = 'model_weights/' + dataset_name + '_simple_CNN'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fig = plot_conf_mat(model, load=True, load_loc='gender_models_trained/simple_CNN.81-0.96.hdf5', \n",
    "#               class_lst = 'Female Male', \n",
    "#               out='model_weights/finalconf_simplecnn.png')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 855 steps, validate for 268 steps\n",
      "Epoch 1/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7035 - accuracy: 0.5180\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.37838, saving model to model_weights/imdb_simple_CNN.01-0.38.hdf5\n",
      "855/855 [==============================] - 570s 666ms/step - loss: 0.7034 - accuracy: 0.5180 - val_loss: 1.1610 - val_accuracy: 0.3784\n",
      "Epoch 2/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7121 - accuracy: 0.5063\n",
      "Epoch 00002: val_accuracy improved from 0.37838 to 0.42420, saving model to model_weights/imdb_simple_CNN.02-0.42.hdf5\n",
      "855/855 [==============================] - 458s 536ms/step - loss: 0.7121 - accuracy: 0.5062 - val_loss: 1.1771 - val_accuracy: 0.4242\n",
      "Epoch 3/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7182 - accuracy: 0.5081\n",
      "Epoch 00003: val_accuracy improved from 0.42420 to 0.76582, saving model to model_weights/imdb_simple_CNN.03-0.77.hdf5\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7181 - accuracy: 0.5081 - val_loss: 1.1832 - val_accuracy: 0.7658\n",
      "Epoch 4/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7236 - accuracy: 0.5060\n",
      "Epoch 00004: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7235 - accuracy: 0.5060 - val_loss: 1.1555 - val_accuracy: 0.7409\n",
      "Epoch 5/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7280 - accuracy: 0.5066\n",
      "Epoch 00005: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7281 - accuracy: 0.5066 - val_loss: 1.1517 - val_accuracy: 0.5876\n",
      "Epoch 6/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7327 - accuracy: 0.5074\n",
      "Epoch 00006: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7327 - accuracy: 0.5074 - val_loss: 1.1570 - val_accuracy: 0.4150\n",
      "Epoch 7/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7372 - accuracy: 0.5050\n",
      "Epoch 00007: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7371 - accuracy: 0.5050 - val_loss: 1.1629 - val_accuracy: 0.7526\n",
      "Epoch 8/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7410 - accuracy: 0.5077\n",
      "Epoch 00008: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 432s 505ms/step - loss: 0.7410 - accuracy: 0.5078 - val_loss: 1.1828 - val_accuracy: 0.7175\n",
      "Epoch 9/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7405 - accuracy: 0.5073\n",
      "Epoch 00009: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7406 - accuracy: 0.5073 - val_loss: 1.1934 - val_accuracy: 0.7039\n",
      "Epoch 10/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7430 - accuracy: 0.5069\n",
      "Epoch 00010: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7430 - accuracy: 0.5069 - val_loss: 1.1575 - val_accuracy: 0.3318\n",
      "Epoch 11/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7449 - accuracy: 0.5060\n",
      "Epoch 00011: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7449 - accuracy: 0.5060 - val_loss: 1.1615 - val_accuracy: 0.6696\n",
      "Epoch 12/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7461 - accuracy: 0.5096\n",
      "Epoch 00012: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7462 - accuracy: 0.5095 - val_loss: 1.2714 - val_accuracy: 0.2341\n",
      "Epoch 13/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7423 - accuracy: 0.5091\n",
      "Epoch 00013: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7424 - accuracy: 0.5090 - val_loss: 1.1586 - val_accuracy: 0.6697\n",
      "Epoch 14/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7440 - accuracy: 0.5128\n",
      "Epoch 00014: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7440 - accuracy: 0.5128 - val_loss: 1.1574 - val_accuracy: 0.3563\n",
      "Epoch 15/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7495 - accuracy: 0.5105\n",
      "Epoch 00015: val_accuracy did not improve from 0.76582\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7495 - accuracy: 0.5105 - val_loss: 1.1529 - val_accuracy: 0.5191\n",
      "Epoch 16/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7577 - accuracy: 0.5104\n",
      "Epoch 00016: val_accuracy improved from 0.76582 to 0.77825, saving model to model_weights/imdb_simple_CNN.16-0.78.hdf5\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.7577 - accuracy: 0.5104 - val_loss: 1.1662 - val_accuracy: 0.7782\n",
      "Epoch 17/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7571 - accuracy: 0.5131\n",
      "Epoch 00017: val_accuracy did not improve from 0.77825\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.7570 - accuracy: 0.5131 - val_loss: 1.1631 - val_accuracy: 0.3084\n",
      "Epoch 18/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7666 - accuracy: 0.5092\n",
      "Epoch 00018: val_accuracy did not improve from 0.77825\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.7665 - accuracy: 0.5092 - val_loss: 1.2654 - val_accuracy: 0.2105\n",
      "Epoch 19/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7866 - accuracy: 0.5083\n",
      "Epoch 00019: val_accuracy did not improve from 0.77825\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.7867 - accuracy: 0.5083 - val_loss: 1.3327 - val_accuracy: 0.2258\n",
      "Epoch 20/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8050 - accuracy: 0.5015\n",
      "Epoch 00020: val_accuracy did not improve from 0.77825\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.8051 - accuracy: 0.5015 - val_loss: 1.4036 - val_accuracy: 0.2546\n",
      "Epoch 21/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8217 - accuracy: 0.5013\n",
      "Epoch 00021: val_accuracy improved from 0.77825 to 0.80562, saving model to model_weights/imdb_simple_CNN.21-0.81.hdf5\n",
      "855/855 [==============================] - 426s 499ms/step - loss: 0.8217 - accuracy: 0.5013 - val_loss: 1.3545 - val_accuracy: 0.8056\n",
      "Epoch 22/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8386 - accuracy: 0.4993\n",
      "Epoch 00022: val_accuracy did not improve from 0.80562\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.8386 - accuracy: 0.4994 - val_loss: 1.4406 - val_accuracy: 0.1653\n",
      "Epoch 23/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.5023\n",
      "Epoch 00023: val_accuracy did not improve from 0.80562\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.8614 - accuracy: 0.5023 - val_loss: 1.3665 - val_accuracy: 0.7896\n",
      "Epoch 24/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8801 - accuracy: 0.5003\n",
      "Epoch 00024: val_accuracy did not improve from 0.80562\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.8803 - accuracy: 0.5003 - val_loss: 1.4594 - val_accuracy: 0.1883\n",
      "Epoch 25/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.8979 - accuracy: 0.5003\n",
      "Epoch 00025: val_accuracy improved from 0.80562 to 0.81709, saving model to model_weights/imdb_simple_CNN.25-0.82.hdf5\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.8980 - accuracy: 0.5003 - val_loss: 1.7494 - val_accuracy: 0.8171\n",
      "Epoch 26/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.9195 - accuracy: 0.4994\n",
      "Epoch 00026: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.9196 - accuracy: 0.4994 - val_loss: 1.6742 - val_accuracy: 0.1888\n",
      "Epoch 27/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.9412 - accuracy: 0.4986\n",
      "Epoch 00027: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.9413 - accuracy: 0.4985 - val_loss: 1.6355 - val_accuracy: 0.8067\n",
      "Epoch 28/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.9613 - accuracy: 0.5002\n",
      "Epoch 00028: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.9614 - accuracy: 0.5003 - val_loss: 1.7510 - val_accuracy: 0.2042\n",
      "Epoch 29/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.9834 - accuracy: 0.4997\n",
      "Epoch 00029: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.9834 - accuracy: 0.4996 - val_loss: 1.7979 - val_accuracy: 0.7796\n",
      "Epoch 30/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 1.0043 - accuracy: 0.5002\n",
      "Epoch 00030: val_accuracy did not improve from 0.81709\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 1.0042 - accuracy: 0.5002 - val_loss: 1.9192 - val_accuracy: 0.2089\n",
      "Epoch 31/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7081 - accuracy: 0.5740\n",
      "Epoch 00031: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7081 - accuracy: 0.5740 - val_loss: 1.1650 - val_accuracy: 0.5772\n",
      "Epoch 32/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7058 - accuracy: 0.5699\n",
      "Epoch 00032: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7058 - accuracy: 0.5700 - val_loss: 1.1652 - val_accuracy: 0.5540\n",
      "Epoch 33/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7065 - accuracy: 0.5708\n",
      "Epoch 00033: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7065 - accuracy: 0.5707 - val_loss: 1.1663 - val_accuracy: 0.5176\n",
      "Epoch 34/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7064 - accuracy: 0.5689\n",
      "Epoch 00034: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7063 - accuracy: 0.5689 - val_loss: 1.1653 - val_accuracy: 0.5480\n",
      "Epoch 35/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7063 - accuracy: 0.5737\n",
      "Epoch 00035: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7064 - accuracy: 0.5737 - val_loss: 1.1653 - val_accuracy: 0.5211\n",
      "Epoch 36/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7065 - accuracy: 0.5726\n",
      "Epoch 00036: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7066 - accuracy: 0.5726 - val_loss: 1.1670 - val_accuracy: 0.5954\n",
      "Epoch 37/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7067 - accuracy: 0.5734\n",
      "Epoch 00037: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 427s 499ms/step - loss: 0.7068 - accuracy: 0.5734 - val_loss: 1.1695 - val_accuracy: 0.6450\n",
      "Epoch 38/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7071 - accuracy: 0.5718\n",
      "Epoch 00038: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7070 - accuracy: 0.5718 - val_loss: 1.1675 - val_accuracy: 0.6358\n",
      "Epoch 39/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7072 - accuracy: 0.5725\n",
      "Epoch 00039: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7070 - accuracy: 0.5726 - val_loss: 1.1682 - val_accuracy: 0.6308\n",
      "Epoch 40/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7074 - accuracy: 0.5713\n",
      "Epoch 00040: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7077 - accuracy: 0.5713 - val_loss: 1.1682 - val_accuracy: 0.5636\n",
      "Epoch 41/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7075 - accuracy: 0.5740\n",
      "Epoch 00041: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7074 - accuracy: 0.5740 - val_loss: 1.1685 - val_accuracy: 0.5384\n",
      "Epoch 42/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7075 - accuracy: 0.5723\n",
      "Epoch 00042: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7074 - accuracy: 0.5723 - val_loss: 1.1727 - val_accuracy: 0.6645\n",
      "Epoch 43/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.5715\n",
      "Epoch 00043: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7077 - accuracy: 0.5714 - val_loss: 1.1688 - val_accuracy: 0.6307\n",
      "Epoch 44/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7083 - accuracy: 0.5706\n",
      "Epoch 00044: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7083 - accuracy: 0.5706 - val_loss: 1.1696 - val_accuracy: 0.5761\n",
      "Epoch 45/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.5725\n",
      "Epoch 00045: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7079 - accuracy: 0.5725 - val_loss: 1.1692 - val_accuracy: 0.5420\n",
      "Epoch 46/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.5735\n",
      "Epoch 00046: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7079 - accuracy: 0.5735 - val_loss: 1.1801 - val_accuracy: 0.6915\n",
      "Epoch 47/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7084 - accuracy: 0.5711\n",
      "Epoch 00047: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7085 - accuracy: 0.5712 - val_loss: 1.1713 - val_accuracy: 0.5147\n",
      "Epoch 48/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7086 - accuracy: 0.5723\n",
      "Epoch 00048: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7085 - accuracy: 0.5723 - val_loss: 1.1721 - val_accuracy: 0.5234\n",
      "Epoch 49/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.5683\n",
      "Epoch 00049: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7088 - accuracy: 0.5683 - val_loss: 1.1817 - val_accuracy: 0.7039\n",
      "Epoch 50/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5702\n",
      "Epoch 00050: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7089 - accuracy: 0.5702 - val_loss: 1.1751 - val_accuracy: 0.5006\n",
      "Epoch 51/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5710\n",
      "Epoch 00051: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7091 - accuracy: 0.5709 - val_loss: 1.1794 - val_accuracy: 0.4440\n",
      "Epoch 52/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5690\n",
      "Epoch 00052: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7095 - accuracy: 0.5690 - val_loss: 1.1800 - val_accuracy: 0.6768\n",
      "Epoch 53/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5720\n",
      "Epoch 00053: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7094 - accuracy: 0.5719 - val_loss: 1.1796 - val_accuracy: 0.6928\n",
      "Epoch 54/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.5731\n",
      "Epoch 00054: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7097 - accuracy: 0.5730 - val_loss: 1.1895 - val_accuracy: 0.7113\n",
      "Epoch 55/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7100 - accuracy: 0.5674\n",
      "Epoch 00055: val_accuracy did not improve from 0.81709\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7101 - accuracy: 0.5674 - val_loss: 1.1752 - val_accuracy: 0.5072\n",
      "Epoch 56/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7087 - accuracy: 0.5753\n",
      "Epoch 00056: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7087 - accuracy: 0.5753 - val_loss: 1.1722 - val_accuracy: 0.5681\n",
      "Epoch 57/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7086 - accuracy: 0.5708\n",
      "Epoch 00057: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7088 - accuracy: 0.5708 - val_loss: 1.1721 - val_accuracy: 0.5691\n",
      "Epoch 58/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7087 - accuracy: 0.5723\n",
      "Epoch 00058: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7087 - accuracy: 0.5723 - val_loss: 1.1717 - val_accuracy: 0.5708\n",
      "Epoch 59/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5731\n",
      "Epoch 00059: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7090 - accuracy: 0.5731 - val_loss: 1.1716 - val_accuracy: 0.5671\n",
      "Epoch 60/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5747\n",
      "Epoch 00060: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 498ms/step - loss: 0.7089 - accuracy: 0.5746 - val_loss: 1.1720 - val_accuracy: 0.5749\n",
      "Epoch 61/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.5705\n",
      "Epoch 00061: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7090 - accuracy: 0.5705 - val_loss: 1.1723 - val_accuracy: 0.5697\n",
      "Epoch 62/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.5746\n",
      "Epoch 00062: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7089 - accuracy: 0.5746 - val_loss: 1.1711 - val_accuracy: 0.5723\n",
      "Epoch 63/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5727\n",
      "Epoch 00063: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7088 - accuracy: 0.5727 - val_loss: 1.1719 - val_accuracy: 0.5625\n",
      "Epoch 64/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5722\n",
      "Epoch 00064: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7091 - accuracy: 0.5722 - val_loss: 1.1719 - val_accuracy: 0.5687\n",
      "Epoch 65/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7088 - accuracy: 0.5710\n",
      "Epoch 00065: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7088 - accuracy: 0.5711 - val_loss: 1.1717 - val_accuracy: 0.5710\n",
      "Epoch 66/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5730\n",
      "Epoch 00066: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7090 - accuracy: 0.5729 - val_loss: 1.1718 - val_accuracy: 0.5680\n",
      "Epoch 67/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5731\n",
      "Epoch 00067: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7089 - accuracy: 0.5731 - val_loss: 1.1727 - val_accuracy: 0.5665\n",
      "Epoch 68/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5730\n",
      "Epoch 00068: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7088 - accuracy: 0.5730 - val_loss: 1.1713 - val_accuracy: 0.5698\n",
      "Epoch 69/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5722\n",
      "Epoch 00069: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7091 - accuracy: 0.5722 - val_loss: 1.1718 - val_accuracy: 0.5654\n",
      "Epoch 70/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5734\n",
      "Epoch 00070: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7089 - accuracy: 0.5734 - val_loss: 1.1725 - val_accuracy: 0.5671\n",
      "Epoch 71/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5744\n",
      "Epoch 00071: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5744 - val_loss: 1.1715 - val_accuracy: 0.5688\n",
      "Epoch 72/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5738\n",
      "Epoch 00072: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5739 - val_loss: 1.1716 - val_accuracy: 0.5746\n",
      "Epoch 73/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.5735\n",
      "Epoch 00073: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7089 - accuracy: 0.5735 - val_loss: 1.1721 - val_accuracy: 0.5648\n",
      "Epoch 74/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5727\n",
      "Epoch 00074: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7090 - accuracy: 0.5726 - val_loss: 1.1730 - val_accuracy: 0.5685\n",
      "Epoch 75/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5743\n",
      "Epoch 00075: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7090 - accuracy: 0.5743 - val_loss: 1.1728 - val_accuracy: 0.5644\n",
      "Epoch 76/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7090 - accuracy: 0.5725\n",
      "Epoch 00076: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7091 - accuracy: 0.5726 - val_loss: 1.1722 - val_accuracy: 0.5700\n",
      "Epoch 77/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5717\n",
      "Epoch 00077: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7092 - accuracy: 0.5717 - val_loss: 1.1729 - val_accuracy: 0.5707\n",
      "Epoch 78/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5750\n",
      "Epoch 00078: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7093 - accuracy: 0.5750 - val_loss: 1.1715 - val_accuracy: 0.5692\n",
      "Epoch 79/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5717\n",
      "Epoch 00079: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5718 - val_loss: 1.1722 - val_accuracy: 0.5691\n",
      "Epoch 80/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5707\n",
      "Epoch 00080: val_accuracy did not improve from 0.81709\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "855/855 [==============================] - 457s 534ms/step - loss: 0.7093 - accuracy: 0.5707 - val_loss: 1.1718 - val_accuracy: 0.5669\n",
      "Epoch 81/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5750\n",
      "Epoch 00081: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5750 - val_loss: 1.1721 - val_accuracy: 0.5722\n",
      "Epoch 82/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5737\n",
      "Epoch 00082: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5738 - val_loss: 1.1724 - val_accuracy: 0.5677\n",
      "Epoch 83/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5739\n",
      "Epoch 00083: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7091 - accuracy: 0.5739 - val_loss: 1.1722 - val_accuracy: 0.5678\n",
      "Epoch 84/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5749\n",
      "Epoch 00084: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7092 - accuracy: 0.5749 - val_loss: 1.1725 - val_accuracy: 0.5662\n",
      "Epoch 85/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5746\n",
      "Epoch 00085: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 437s 511ms/step - loss: 0.7092 - accuracy: 0.5746 - val_loss: 1.1728 - val_accuracy: 0.5657\n",
      "Epoch 86/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5728\n",
      "Epoch 00086: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 429s 502ms/step - loss: 0.7092 - accuracy: 0.5728 - val_loss: 1.1720 - val_accuracy: 0.5675\n",
      "Epoch 87/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5729\n",
      "Epoch 00087: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 480s 561ms/step - loss: 0.7091 - accuracy: 0.5729 - val_loss: 1.1725 - val_accuracy: 0.5644\n",
      "Epoch 88/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5758\n",
      "Epoch 00088: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 526s 615ms/step - loss: 0.7094 - accuracy: 0.5757 - val_loss: 1.1728 - val_accuracy: 0.5622\n",
      "Epoch 89/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5743\n",
      "Epoch 00089: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 459s 537ms/step - loss: 0.7092 - accuracy: 0.5743 - val_loss: 1.1724 - val_accuracy: 0.5663\n",
      "Epoch 90/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7092 - accuracy: 0.5735\n",
      "Epoch 00090: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 430s 503ms/step - loss: 0.7092 - accuracy: 0.5735 - val_loss: 1.1725 - val_accuracy: 0.5642\n",
      "Epoch 91/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5737\n",
      "Epoch 00091: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5737 - val_loss: 1.1723 - val_accuracy: 0.5662\n",
      "Epoch 92/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5736\n",
      "Epoch 00092: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7094 - accuracy: 0.5737 - val_loss: 1.1721 - val_accuracy: 0.5633\n",
      "Epoch 93/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7091 - accuracy: 0.5742\n",
      "Epoch 00093: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7092 - accuracy: 0.5742 - val_loss: 1.1726 - val_accuracy: 0.5644\n",
      "Epoch 94/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5735\n",
      "Epoch 00094: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5735 - val_loss: 1.1724 - val_accuracy: 0.5673\n",
      "Epoch 95/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5729\n",
      "Epoch 00095: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5729 - val_loss: 1.1719 - val_accuracy: 0.5664\n",
      "Epoch 96/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5733\n",
      "Epoch 00096: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7094 - accuracy: 0.5733 - val_loss: 1.1720 - val_accuracy: 0.5633\n",
      "Epoch 97/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7093 - accuracy: 0.5748\n",
      "Epoch 00097: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 425s 497ms/step - loss: 0.7093 - accuracy: 0.5748 - val_loss: 1.1724 - val_accuracy: 0.5592\n",
      "Epoch 98/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5727\n",
      "Epoch 00098: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 426s 498ms/step - loss: 0.7092 - accuracy: 0.5727 - val_loss: 1.1723 - val_accuracy: 0.5607\n",
      "Epoch 99/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7095 - accuracy: 0.5720\n",
      "Epoch 00099: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 431s 504ms/step - loss: 0.7094 - accuracy: 0.5721 - val_loss: 1.1725 - val_accuracy: 0.5595\n",
      "Epoch 100/100\n",
      "854/855 [============================>.] - ETA: 0s - loss: 0.7094 - accuracy: 0.5747\n",
      "Epoch 00100: val_accuracy did not improve from 0.81709\n",
      "855/855 [==============================] - 427s 500ms/step - loss: 0.7095 - accuracy: 0.5747 - val_loss: 1.1725 - val_accuracy: 0.5585\n"
     ]
    }
   ],
   "source": [
    "history_simpleCNN = model.fit(train_generator, steps_per_epoch=nb_train_samples // batch_size, \n",
    "                                 epochs=num_epochs, validation_data=validation_generator, \n",
    "                                 validation_steps=nb_validation_samples // batch_size, verbose=1,\n",
    "                                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plot_loss_acc(history_simpleCNN)\n",
    "fig.savefig('simpleCNN.gender.losscurve.png')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## MODEL: mini XCEPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# mini XCEPTION callbacks\n",
    "log_file_path = 'logs/' + dataset_name + '_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "\n",
    "trained_models_path = 'model_weights_gender_miniXception/' + dataset_name + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#mini_XCEPTION\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import SeparableConv2D, MaxPooling2D\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "l2_regularization=0.06\n",
    "\n",
    "regularization = l2(l2_regularization)\n",
    "\n",
    "# base\n",
    "img_input = Input(input_shape)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "           use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
    "           use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# module 1\n",
    "residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
    "                  padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(16, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "# module 2\n",
    "residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
    "                  padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "# module 3\n",
    "residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
    "                  padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "# module 4\n",
    "residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
    "                  padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',\n",
    "                    kernel_regularizer=regularization,\n",
    "                    use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "\n",
    "x = Conv2D(num_classes, (3, 3),\n",
    "           # kernel_regularizer=regularization,\n",
    "           padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Activation('softmax', name='predictions')(x)\n",
    "\n",
    "model = Model(img_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# fig = plot_conf_mat(model, load=True, load_loc='gender_models_trained/gender_mini_XCEPTION.21-0.95.hdf5', \n",
    "#               class_lst = 'Female Male', \n",
    "#               out='model_weights/finalconf_genderxecption.png')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 855 steps, validate for 268 steps\n",
      "Epoch 1/100\n",
      " 64/855 [=>............................] - ETA: 5:31 - loss: 1.3596 - accuracy: 0.4910"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a516ab74e1c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_validation_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                  callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "history_mini_Xception = model.fit(train_generator, steps_per_epoch=nb_train_samples // batch_size, \n",
    "                                 epochs=num_epochs, validation_data=validation_generator, \n",
    "                                 validation_steps=nb_validation_samples // batch_size, verbose=1,\n",
    "                                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fig = plot_loss_acc(history_mini_Xception)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "## MODEL: Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# raw_df = pd.read_csv('data/fer2013_and_plus.csv')\n",
    "# pixels = raw_df['pixels'].tolist()\n",
    "# faces = []\n",
    "# for pixel_sequence in pixels:\n",
    "#     face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "#     face = np.asarray(face).reshape(image_width, image_height)\n",
    "#     face = cv2.resize(face.astype('uint8'), image_size)\n",
    "#     faces.append(face.astype('float32'))\n",
    "# faces = np.asarray(faces)\n",
    "# faces = np.expand_dims(faces, -1)\n",
    "# emotions = pd.get_dummies(raw_df[dataset_name]).as_matrix()\n",
    "\n",
    "# faces = preprocess_input(faces)\n",
    "# num_samples, num_classes = emotions.shape\n",
    "# train_data, val_data = split_data(faces, emotions, validation_split=0.2)\n",
    "# train_faces, train_emotions = train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## MODEL: ResNet50 Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/9d/d5772f94e31431cdb56a8bb2c34d8839bb7d7621f2a5959f4ef43207d7ac/tensorflow_hub-0.8.0-py2.py3-none-any.whl (101kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (1.16.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (3.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\s\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub>=0.6.0) (41.4.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow_hub>=0.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-10-1ac7f3e629d5>:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "#print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4 with input size (64, 64)\n"
     ]
    }
   ],
   "source": [
    "# pixels = 64\n",
    "# module_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "# MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "# IMAGE_SIZE = (pixels, pixels)\n",
    "# print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
    "\n",
    "# BATCH_SIZE = 32 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# do_fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,568,898\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print(\"Building model with\", MODULE_HANDLE)\n",
    "# model = tf.keras.Sequential([\n",
    "#     # Explicitly define the input shape so the model can be properly\n",
    "#     # loaded by the TFLiteConverter\n",
    "#     tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "#     hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "#     tf.keras.layers.Dropout(rate=0.2),\n",
    "#     tf.keras.layers.Dense(train_generator.num_classes,\n",
    "#                           kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "# ])\n",
    "# model.build((None,)+IMAGE_SIZE+(3,))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x2ce7f8288c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb\n",
    "# https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning\n",
    "# https://github.com/imdeepmind/processed-imdb-wiki-dataset/blob/master/gender.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "175\n"
     ]
    }
   ],
   "source": [
    "top_model_weights_path = '../bottleneck_fc_model.h5'\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "# build the resnet50 network\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64,64,3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(512, activation='relu'))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(128, activation='relu'))\n",
    "top_model.add(Dropout(0.2))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(len(base_model.layers))\n",
    "for layer in base_model.layers[:175]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[175:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "#top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# resnet summary\n",
    "# base_model.summary()\n",
    "# i=0\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "#     i = i+1\n",
    "#     print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "# other models use this\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "# add the model on top of the convolutional base\n",
    "model.add(top_model)\n",
    "\n",
    "\n",
    "# set the first 170 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "\n",
    "\n",
    "    \n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=5e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "log_file_path = 'logs/' + dataset_name + '_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "\n",
    "trained_models_path = 'model_weights_gender_resnet/' + dataset_name + '_resnet50_finetune'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# model.trainable=True\n",
    "\n",
    "len(model.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2673 steps, validate for 839 steps\n",
      "Epoch 1/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5771 - accuracy: 0.6974\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.27199, saving model to model_weights_gender_resnet/imdb_resnet50_finetune.01-0.27.hdf5\n",
      "2673/2673 [==============================] - 602s 225ms/step - loss: 0.5771 - accuracy: 0.6974 - val_loss: 0.7091 - val_accuracy: 0.2720\n",
      "Epoch 2/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7383\n",
      "Epoch 00002: val_accuracy did not improve from 0.27199\n",
      "2673/2673 [==============================] - 602s 225ms/step - loss: 0.5241 - accuracy: 0.7383 - val_loss: 0.7147 - val_accuracy: 0.2518\n",
      "Epoch 3/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7486\n",
      "Epoch 00003: val_accuracy did not improve from 0.27199\n",
      "2673/2673 [==============================] - 601s 225ms/step - loss: 0.5104 - accuracy: 0.7486 - val_loss: 0.7174 - val_accuracy: 0.2451\n",
      "Epoch 4/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5012 - accuracy: 0.7557\n",
      "Epoch 00004: val_accuracy did not improve from 0.27199\n",
      "2673/2673 [==============================] - 599s 224ms/step - loss: 0.5012 - accuracy: 0.7557 - val_loss: 0.8189 - val_accuracy: 0.1778\n",
      "Epoch 5/100\n",
      "2031/2673 [=====================>........] - ETA: 1:51 - loss: 0.4972 - accuracy: 0.7569"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 767\u001b[1;33m       \u001b[1;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    768\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-7e8321f6388e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                  \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_validation_samples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                  callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m                       total_epochs=1)\n\u001b[0;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[1;32m--> 397\u001b[1;33m                                  prefix='val_')\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[1;34m(self, epoch, mode)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;31m# Epochs only apply to `fit`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m       \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    990\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 992\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    993\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m       \u001b[1;31m# For multi-worker training, back up the weights and current training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1009\u001b[0m                   int) or self.epochs_since_last_save >= self.period:\n\u001b[0;32m   1010\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs_since_last_save\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_file_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_get_file_path\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     if not self.model._in_multi_worker_mode(\n\u001b[0;32m   1054\u001b[0m     ) or multi_worker_util.should_save_checkpoint():\n\u001b[1;32m-> 1055\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1056\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[1;31m# If this is multi-worker training, and this worker should not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    }
   ],
   "source": [
    "#fit\n",
    "resnet_finetune = model.fit(train_generator, steps_per_epoch=nb_train_samples // batch_size, \n",
    "                                 epochs=num_epochs, validation_data=validation_generator, \n",
    "                                 validation_steps=nb_validation_samples // batch_size, verbose=1,\n",
    "                                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# resnet sample code to test\n",
    "# img_path = '1280px-African_Bush_Elephant.jpg'\n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "# x = image.img_to_array(img)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = preprocess_input(x)\n",
    "\n",
    "# preds = model.predict(x)\n",
    "# # decode the results into a list of tuples (class, description, probability)\n",
    "# # (one such list for each sample in the batch)\n",
    "# print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## MODEL: ResNet50 Bottlenecked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/9d/d5772f94e31431cdb56a8bb2c34d8839bb7d7621f2a5959f4ef43207d7ac/tensorflow_hub-0.8.0-py2.py3-none-any.whl (101kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (1.16.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\s\\anaconda3\\lib\\site-packages (from tensorflow_hub>=0.6.0) (3.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\s\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow_hub>=0.6.0) (41.4.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tensorflow_hub>=0.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.1.0\n",
      "WARNING:tensorflow:From <ipython-input-11-1ac7f3e629d5>:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow_hub as hub\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "#print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4 with input size (64, 64)\n"
     ]
    }
   ],
   "source": [
    "# pixels = 64\n",
    "# module_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "\n",
    "# MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "# IMAGE_SIZE = (pixels, pixels)\n",
    "# print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
    "\n",
    "# BATCH_SIZE = 32 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# do_fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,568,898\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print(\"Building model with\", MODULE_HANDLE)\n",
    "# model = tf.keras.Sequential([\n",
    "#     # Explicitly define the input shape so the model can be properly\n",
    "#     # loaded by the TFLiteConverter\n",
    "#     tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "#     hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "#     tf.keras.layers.Dropout(rate=0.2),\n",
    "#     tf.keras.layers.Dense(train_generator.num_classes,\n",
    "#                           kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "# ])\n",
    "# model.build((None,)+IMAGE_SIZE+(3,))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "top_model_weights_path = '../bottleneck_fc_model.h5'\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "\n",
    "# build the resnet50 network\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64,64,3))\n",
    "print('Model loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-faf99bd1fcb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m bottleneck_features_validation = base_model.predict(\n\u001b[1;32m----> 6\u001b[1;33m     val_generator, steps=nb_validation_samples // batch_size)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_generator' is not defined"
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = base_model.predict(\n",
    "    train_generator, steps=nb_train_samples // batch_size)\n",
    "#np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bottleneck_features_validation = base_model.predict(\n",
    "    validation_generator, steps=nb_validation_samples // batch_size)\n",
    "#np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "#top_model.load_weights(top_model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# resnet summary\n",
    "# base_model.summary()\n",
    "# i=0\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "#     i = i+1\n",
    "#     print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=5e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#callbacks\n",
    "log_file_path = 'logs/' + dataset_name + '_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "\n",
    "trained_models_path = 'model_weights_gender_resnet/' + dataset_name + '_resnet50_finetune'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_accuracy:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.trainable=True\n",
    "\n",
    "len(model.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2673 steps, validate for 839 steps\n",
      "Epoch 1/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5771 - accuracy: 0.6974\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.27199, saving model to model_weights_gender_resnet/imdb_resnet50_finetune.01-0.27.hdf5\n",
      "2673/2673 [==============================] - 602s 225ms/step - loss: 0.5771 - accuracy: 0.6974 - val_loss: 0.7091 - val_accuracy: 0.2720\n",
      "Epoch 2/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5241 - accuracy: 0.7383\n",
      "Epoch 00002: val_accuracy did not improve from 0.27199\n",
      "2673/2673 [==============================] - 602s 225ms/step - loss: 0.5241 - accuracy: 0.7383 - val_loss: 0.7147 - val_accuracy: 0.2518\n",
      "Epoch 3/100\n",
      "2672/2673 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7486"
     ]
    }
   ],
   "source": [
    "#fit\n",
    "resnet_finetune = model.fit(train_generator, steps_per_epoch=nb_train_samples // batch_size, \n",
    "                                 epochs=num_epochs, validation_data=validation_generator, \n",
    "                                 validation_steps=nb_validation_samples // batch_size, verbose=1,\n",
    "                                 callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# resnet sample code to test\n",
    "# img_path = '1280px-African_Bush_Elephant.jpg'\n",
    "# img = image.load_img(img_path, target_size=(224, 224))\n",
    "# x = image.img_to_array(img)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = preprocess_input(x)\n",
    "\n",
    "# preds = model.predict(x)\n",
    "# # decode the results into a list of tuples (class, description, probability)\n",
    "# # (one such list for each sample in the batch)\n",
    "# print('Predicted:', decode_predictions(preds, top=3)[0])\n",
    "# # Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
